{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868e5ec2-3d6e-4378-bdb3-a1848a51a20a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_362-362\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_362-362-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abb216-914a-4f50-abc0-b4c07a4f401d",
   "metadata": {},
   "source": [
    "# Java instalado en el cluster\n",
    "**openjdk version \"1.8.0_362\"**\n",
    "\n",
    "**OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~22.04-b09)**\n",
    "\n",
    "**OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b177a295-14a5-4eda-a12f-c2af55a467e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.13\n",
      "23/09/14 10:36:43 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/09/14 10:36:43 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:218)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:923)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/14 10:36:43 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.174.108: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/09/14 10:36:43 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.23.198: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb408a-ccf5-4739-858f-f2bf1991ad03",
   "metadata": {},
   "source": [
    "# Python\n",
    "Python 3.8.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c298d264-4af8-44e0-bf85-a4e304a25b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.3.1 in /home/coder/miniconda3/envs/demml/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/coder/miniconda3/envs/demml/lib/python3.8/site-packages (from pyspark==3.3.1) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09debb5-e2ed-4d15-929d-edda9733cf29",
   "metadata": {},
   "source": [
    "# pyspark\n",
    "**Requirement already satisfied: pyspark==3.3.1 in /opt/conda/lib/python3.8/site-packages (3.3.1)**\n",
    "\n",
    "**Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.8/site-packages (from pyspark==3.3.1) (0.10.9.5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d6ea50-bc3d-4bbb-ae76-febb39c05bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/14 10:23:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.23.255:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master-0.spark-headless.demml.svc.cluster.local:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iris_naive_bayes_classification_pyspark_EDU_4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mpyspark.sql.session.SparkSession\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f1e5bfa1be0\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "dst_lh_url  = os.environ.get('DST_LH_URL', 'spark://spark-master-0.spark-headless.demml.svc.cluster.local:7077')\n",
    "dst_lh_appn = os.environ.get('DST_LH_APPN', 'iris_naive_bayes_classification_pyspark_EDU_6')\n",
    "my_pod_ip = subprocess.run(['hostname', '-I'], stdout=subprocess.PIPE).stdout.decode('utf-8').strip(' \\n\\t')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(dst_lh_url) \\\n",
    "    .appName(dst_lh_appn) \\\n",
    "    .config('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.1.0,org.mariadb.jdbc:mariadb-java-client:2.3.0,org.postgresql:postgresql:42.5.4,software.amazon.awssdk:bundle:2.17.257,software.amazon.awssdk:url-connection-client:2.17.257,software.amazon.awssdk:s3:2.17.257,software.amazon.awssdk:iam:2.17.257,org.apache.hadoop:hadoop-aws:3.2.3') \\\n",
    "    .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.iceberg.spark.SparkSessionCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.catalog-impl','org.apache.iceberg.jdbc.JdbcCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.uri','jdbc:postgresql://postgresql:5432/iceberg') \\\n",
    "    .config('spark.driver.host', my_pod_ip) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.jdbc.useSSL','false') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.jdbc.user','iceberg') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.jdbc.password','iceberg') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.warehouse','s3a://warehouse/') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.endpoint','http://minio:9000') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.defaultCatalog','spark_catalog') \\\n",
    "    .config('spark.hadoop.fs.s3a.endpoint','http://minio:9000') \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key','admin') \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key','t4bl4red0nd4') \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access','true') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086372a4-b75e-460c-8613-39d44aa0fb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 256\n",
      "drwxrwsr-x. 3 coder coder   4096 Sep 14 10:21 .\n",
      "drwxrwsr-x. 8 coder coder   4096 Sep 13 09:41 ..\n",
      "-rw-rw-r--. 1 coder coder      0 Sep 13 07:01 .gitkeep\n",
      "drwxrwsr-x. 2 coder coder   4096 Sep 13 09:41 .ipynb_checkpoints\n",
      "-rw-rw-r--. 1 coder coder  21440 Sep 13 12:10 iris-naive-bayes-classification-with-pyspark.ipynb\n",
      "-rw-r--r--. 1 coder coder   3858 Sep 14 10:18 iris.csv\n",
      "-rw-rw-r--. 1 coder coder 218836 Sep 14 10:21 test_conexion_spark.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6c8642-760b-4f07-a22e-32abed8a094a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/14 10:23:13 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.21.174.108 executor 1): java.io.FileNotFoundException: \n",
      "File file:/home/coder/eromero/iris-pyspark/notebooks/iris.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/09/14 10:23:14 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 df = spark.read.csv(<span style=\"color: #808000; text-decoration-color: #808000\">\"iris.csv\"</span>, header=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, inferSchema=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">readwriter.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">535</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">csv</span>                                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 532 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>path = [path]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 533 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(path) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 534 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._spark._sc._jvm <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 535 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._df(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._jreader.csv(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._spark._sc._jvm.PythonUtils.toSeq(pat  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 536 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(path, RDD):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 537 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 538 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">func</span>(iterator):                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">java_gateway.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1321</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>proto.END_COMMAND_PART                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>answer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client.send_command(command)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1321 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_value = get_return_value(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>answer, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.target_id, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> temp_arg <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> temp_args:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">capture_sql_exception</span>(f: Callable[..., Any]) -&gt; Callable[..., Any]:                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>(*a: Any, **kw: Any) -&gt; Any:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*a, **kw)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> Py4JJavaError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>converted = convert_exception(e.java_exception)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(converted, UnknownException):                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">protocol.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">326</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_return_value</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span> = answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>value = OUTPUT_CONVERTER[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>](answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:], gateway_client)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">325 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] == REFERENCE_TYPE:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>326 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> Py4JJavaError(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">327 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"An error occurred while calling {0}{1}{2}.\\n\"</span>.                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">328 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">format</span>(target_id, <span style=\"color: #808000; text-decoration-color: #808000\">\".\"</span>, name), value)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">329 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Py4JJavaError: </span>An error occurred while calling o40.csv.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> failed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> times, most \n",
       "recent failure: Lost task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> <span style=\"font-weight: bold\">(</span>TID <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">172.21.174.108</span> executor <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/notebooks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Driver stacktrace:\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2672</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2608</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ResizableArray.foreach</span><span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.mutable.ResizableArray.foreach$<span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:55</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ArrayBuffer.foreach</span><span style=\"font-weight: bold\">(</span>ArrayBuffer.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.abortStage</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.foreach</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:407</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2860</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2802</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2791</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.EventLoop$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.run</span><span style=\"font-weight: bold\">(</span>EventLoop.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.runJob</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:952</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2249</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2268</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:506</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:459</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.CollectLimitExec.executeCollect</span><span style=\"font-weight: bold\">(</span>limit.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:48</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.collectFromPlan</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3868</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3858</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.QueryExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withInternalError</span><span style=\"font-weight: bold\">(</span>QueryExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:510</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">6</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:109</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withSQLConfPropagated</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:169</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:95</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.SparkSession.withActive</span><span style=\"font-weight: bold\">(</span>SparkSession.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:779</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withNewExecutionId</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:64</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.withAction</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.head</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.take</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3084</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.infer</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:112</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:65</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVFileFormat.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">11</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.orElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:447</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:207</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.resolveRelation</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:411</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.loadV1Source</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.getOrElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:189</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.csv</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:537</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.NativeMethodAccessorImpl.invoke0</span><span style=\"font-weight: bold\">(</span>Native Method<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.NativeMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>NativeMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.DelegatingMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>DelegatingMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.reflect.Method.invoke</span><span style=\"font-weight: bold\">(</span>Method.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:498</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.MethodInvoker.invoke</span><span style=\"font-weight: bold\">(</span>MethodInvoker.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:244</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.ReflectionEngine.invoke</span><span style=\"font-weight: bold\">(</span>ReflectionEngine.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:357</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.Gateway.invoke</span><span style=\"font-weight: bold\">(</span>Gateway.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:282</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.AbstractCommand.invokeMethod</span><span style=\"font-weight: bold\">(</span>AbstractCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:132</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.CallCommand.execute</span><span style=\"font-weight: bold\">(</span>CallCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:79</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.waitForCommands</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.run</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:106</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/notebooks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> more\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 df = spark.read.csv(\u001b[33m\"\u001b[0m\u001b[33miris.csv\u001b[0m\u001b[33m\"\u001b[0m, header=\u001b[94mTrue\u001b[0m, inferSchema=\u001b[94mTrue\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mreadwriter.py\u001b[0m:\u001b[94m535\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mcsv\u001b[0m                                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 532 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath = [path]                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 533 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mtype\u001b[0m(path) == \u001b[96mlist\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 534 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mself\u001b[0m._spark._sc._jvm \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 535 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._df(\u001b[96mself\u001b[0m._jreader.csv(\u001b[96mself\u001b[0m._spark._sc._jvm.PythonUtils.toSeq(pat  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 536 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(path, RDD):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 537 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 538 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfunc\u001b[0m(iterator):                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mjava_gateway.py\u001b[0m:\u001b[94m1321\u001b[0m in       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1318 \u001b[0m\u001b[2m│   │   │   \u001b[0mproto.END_COMMAND_PART                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1319 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1320 \u001b[0m\u001b[2m│   │   \u001b[0manswer = \u001b[96mself\u001b[0m.gateway_client.send_command(command)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1321 \u001b[2m│   │   \u001b[0mreturn_value = get_return_value(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   │   \u001b[0manswer, \u001b[96mself\u001b[0m.gateway_client, \u001b[96mself\u001b[0m.target_id, \u001b[96mself\u001b[0m.name)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m temp_arg \u001b[95min\u001b[0m temp_args:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m190\u001b[0m in \u001b[92mdeco\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcapture_sql_exception\u001b[0m(f: Callable[..., Any]) -> Callable[..., Any]:                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdeco\u001b[0m(*a: Any, **kw: Any) -> Any:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m190 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m f(*a, **kw)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m Py4JJavaError \u001b[94mas\u001b[0m e:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   │   \u001b[0mconverted = convert_exception(e.java_exception)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(converted, UnknownException):                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mprotocol.py\u001b[0m:\u001b[94m326\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_return_value\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m = answer[\u001b[94m1\u001b[0m]                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue = OUTPUT_CONVERTER[\u001b[96mtype\u001b[0m](answer[\u001b[94m2\u001b[0m:], gateway_client)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m answer[\u001b[94m1\u001b[0m] == REFERENCE_TYPE:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m326 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m Py4JJavaError(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m327 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mAn error occurred while calling \u001b[0m\u001b[33m{0}\u001b[0m\u001b[33m{1}\u001b[0m\u001b[33m{2}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m.                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m328 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mformat\u001b[0m(target_id, \u001b[33m\"\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m, name), value)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m329 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mPy4JJavaError: \u001b[0mAn error occurred while calling o40.csv.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task \u001b[1;36m0\u001b[0m in stage \u001b[1;36m0.0\u001b[0m failed \u001b[1;36m4\u001b[0m times, most \n",
       "recent failure: Lost task \u001b[1;36m0.3\u001b[0m in stage \u001b[1;36m0.0\u001b[0m \u001b[1m(\u001b[0mTID \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;92m172.21.174.108\u001b[0m executor \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/notebooks/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Driver stacktrace:\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2672\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2608\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ResizableArray.foreach\u001b[0m\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.mutable.ResizableArray.foreach$\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:55\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ArrayBuffer.foreach\u001b[0m\u001b[1m(\u001b[0mArrayBuffer.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.abortStage\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;36m1\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.foreach\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:407\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2860\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2802\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2791\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.EventLoop$$anon$\u001b[1;35m1.run\u001b[0m\u001b[1m(\u001b[0mEventLoop.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.runJob\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:952\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2249\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2268\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:506\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:459\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.CollectLimitExec.executeCollect\u001b[0m\u001b[1m(\u001b[0mlimit.scal\u001b[1;92ma:48\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.collectFromPlan\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3868\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3858\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.QueryExecution$\u001b[1;35m.withInternalError\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:510\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m6\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:109\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withSQLConfPropagated\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:169\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:95\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.SparkSession.withActive\u001b[0m\u001b[1m(\u001b[0mSparkSession.scal\u001b[1;92ma:779\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withNewExecutionId\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:64\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.withAction\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.head\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.take\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3084\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$\u001b[1;35m.infer\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:112\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:65\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVFileFormat.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$\u001b[1;35m11\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.orElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:447\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:207\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.resolveRelation\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:411\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.loadV1Source\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.getOrElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:189\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.csv\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:537\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.NativeMethodAccessorImpl.invoke0\u001b[0m\u001b[1m(\u001b[0mNative Method\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.NativeMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mNativeMethodAccessorImpl.jav\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.DelegatingMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mDelegatingMethodAccessorImpl.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.reflect.Method.invoke\u001b[0m\u001b[1m(\u001b[0mMethod.jav\u001b[1;92ma:498\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.MethodInvoker.invoke\u001b[0m\u001b[1m(\u001b[0mMethodInvoker.jav\u001b[1;92ma:244\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.ReflectionEngine.invoke\u001b[0m\u001b[1m(\u001b[0mReflectionEngine.jav\u001b[1;92ma:357\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.Gateway.invoke\u001b[0m\u001b[1m(\u001b[0mGateway.jav\u001b[1;92ma:282\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.AbstractCommand.invokeMethod\u001b[0m\u001b[1m(\u001b[0mAbstractCommand.jav\u001b[1;92ma:132\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.CallCommand.execute\u001b[0m\u001b[1m(\u001b[0mCallCommand.jav\u001b[1;92ma:79\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.waitForCommands\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.run\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:106\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/notebooks/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[33m...\u001b[0m \u001b[1;36m1\u001b[0m more\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"iris.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb3168-9cb3-4aee-bbf4-50a536d4798d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0721db3-8841-45b5-9781-9bac205115da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (iris_pyspark)",
   "language": "python",
   "name": "kedro_iris_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
