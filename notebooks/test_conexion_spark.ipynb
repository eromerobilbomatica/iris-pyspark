{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "868e5ec2-3d6e-4378-bdb3-a1848a51a20a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_382\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~20.04.1-b05)\n",
      "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c298d264-4af8-44e0-bf85-a4e304a25b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.3.1 in /home/coder/miniconda3/envs/demml/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/coder/miniconda3/envs/demml/lib/python3.8/site-packages (from pyspark==3.3.1) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98d6ea50-bc3d-4bbb-ae76-febb39c05bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.23.206:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master-0.spark-headless.demml.svc.cluster.local:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iris_naive_bayes_classification_pyspark_EDU_4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mpyspark.sql.session.SparkSession\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f3fff879070\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "dst_lh_url  = os.environ.get('DST_LH_URL', 'spark://spark-master-0.spark-headless.demml.svc.cluster.local:7077')\n",
    "dst_lh_appn = os.environ.get('DST_LH_APPN', 'iris_naive_bayes_classification_pyspark_EDU_6')\n",
    "my_pod_ip = subprocess.run(['hostname', '-I'], stdout=subprocess.PIPE).stdout.decode('utf-8').strip(' \\n\\t')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(dst_lh_url) \\\n",
    "    .appName(dst_lh_appn) \\\n",
    "    .config('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.1.0,org.mariadb.jdbc:mariadb-java-client:2.3.0,org.postgresql:postgresql:42.5.4,software.amazon.awssdk:bundle:2.17.257,software.amazon.awssdk:url-connection-client:2.17.257,software.amazon.awssdk:s3:2.17.257,software.amazon.awssdk:iam:2.17.257,org.apache.hadoop:hadoop-aws:3.2.3') \\\n",
    "    .config('spark.driver.host', my_pod_ip) \\\n",
    "    .config('spark.scheduler.mode', 'FAIR') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0b6c8642-760b-4f07-a22e-32abed8a094a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 12:59:21 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 8) (172.21.23.198 executor 0): java.io.FileNotFoundException: \n",
      "File file:/home/coder/eromero/iris-pyspark/data/01_raw/iris.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/09/13 12:59:21 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 df = spark.read.csv(<span style=\"color: #808000; text-decoration-color: #808000\">\"/home/coder/eromero/iris-pyspark/data/01_raw/iris.csv\"</span>, header=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">readwriter.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">535</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">csv</span>                                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 532 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>path = [path]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 533 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(path) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 534 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._spark._sc._jvm <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 535 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._df(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._jreader.csv(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._spark._sc._jvm.PythonUtils.toSeq(pat  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 536 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(path, RDD):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 537 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 538 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">func</span>(iterator):                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">java_gateway.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1321</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>proto.END_COMMAND_PART                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>answer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client.send_command(command)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1321 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_value = get_return_value(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>answer, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.target_id, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> temp_arg <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> temp_args:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">capture_sql_exception</span>(f: Callable[..., Any]) -&gt; Callable[..., Any]:                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>(*a: Any, **kw: Any) -&gt; Any:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*a, **kw)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> Py4JJavaError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>converted = convert_exception(e.java_exception)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(converted, UnknownException):                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">protocol.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">326</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_return_value</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span> = answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>value = OUTPUT_CONVERTER[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>](answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:], gateway_client)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">325 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] == REFERENCE_TYPE:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>326 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> Py4JJavaError(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">327 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"An error occurred while calling {0}{1}{2}.\\n\"</span>.                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">328 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">format</span>(target_id, <span style=\"color: #808000; text-decoration-color: #808000\">\".\"</span>, name), value)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">329 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Py4JJavaError: </span>An error occurred while calling o540.csv.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> failed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> times, most \n",
       "recent failure: Lost task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>TID <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">172.21.174.108</span> executor <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Driver stacktrace:\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2672</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2608</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ResizableArray.foreach</span><span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.mutable.ResizableArray.foreach$<span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:55</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ArrayBuffer.foreach</span><span style=\"font-weight: bold\">(</span>ArrayBuffer.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.abortStage</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.foreach</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:407</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2860</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2802</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2791</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.EventLoop$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.run</span><span style=\"font-weight: bold\">(</span>EventLoop.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.runJob</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:952</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2249</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2268</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:506</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:459</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.CollectLimitExec.executeCollect</span><span style=\"font-weight: bold\">(</span>limit.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:48</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.collectFromPlan</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3868</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3858</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.QueryExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withInternalError</span><span style=\"font-weight: bold\">(</span>QueryExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:510</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">6</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:109</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withSQLConfPropagated</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:169</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:95</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.SparkSession.withActive</span><span style=\"font-weight: bold\">(</span>SparkSession.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:779</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withNewExecutionId</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:64</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.withAction</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.head</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.take</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3084</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.infer</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:112</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:65</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVFileFormat.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">11</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.orElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:447</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:207</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.resolveRelation</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:411</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.loadV1Source</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.getOrElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:189</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.csv</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:537</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.NativeMethodAccessorImpl.invoke0</span><span style=\"font-weight: bold\">(</span>Native Method<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.NativeMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>NativeMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.DelegatingMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>DelegatingMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.reflect.Method.invoke</span><span style=\"font-weight: bold\">(</span>Method.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:498</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.MethodInvoker.invoke</span><span style=\"font-weight: bold\">(</span>MethodInvoker.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:244</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.ReflectionEngine.invoke</span><span style=\"font-weight: bold\">(</span>ReflectionEngine.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:357</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.Gateway.invoke</span><span style=\"font-weight: bold\">(</span>Gateway.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:282</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.AbstractCommand.invokeMethod</span><span style=\"font-weight: bold\">(</span>AbstractCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:132</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.CallCommand.execute</span><span style=\"font-weight: bold\">(</span>CallCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:79</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.waitForCommands</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.run</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:106</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> more\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 df = spark.read.csv(\u001b[33m\"\u001b[0m\u001b[33m/home/coder/eromero/iris-pyspark/data/01_raw/iris.csv\u001b[0m\u001b[33m\"\u001b[0m, header=\u001b[94mTrue\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mreadwriter.py\u001b[0m:\u001b[94m535\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mcsv\u001b[0m                                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 532 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath = [path]                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 533 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mtype\u001b[0m(path) == \u001b[96mlist\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 534 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mself\u001b[0m._spark._sc._jvm \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 535 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._df(\u001b[96mself\u001b[0m._jreader.csv(\u001b[96mself\u001b[0m._spark._sc._jvm.PythonUtils.toSeq(pat  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 536 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(path, RDD):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 537 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 538 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfunc\u001b[0m(iterator):                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mjava_gateway.py\u001b[0m:\u001b[94m1321\u001b[0m in       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1318 \u001b[0m\u001b[2m│   │   │   \u001b[0mproto.END_COMMAND_PART                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1319 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1320 \u001b[0m\u001b[2m│   │   \u001b[0manswer = \u001b[96mself\u001b[0m.gateway_client.send_command(command)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1321 \u001b[2m│   │   \u001b[0mreturn_value = get_return_value(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   │   \u001b[0manswer, \u001b[96mself\u001b[0m.gateway_client, \u001b[96mself\u001b[0m.target_id, \u001b[96mself\u001b[0m.name)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m temp_arg \u001b[95min\u001b[0m temp_args:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m190\u001b[0m in \u001b[92mdeco\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcapture_sql_exception\u001b[0m(f: Callable[..., Any]) -> Callable[..., Any]:                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdeco\u001b[0m(*a: Any, **kw: Any) -> Any:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m190 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m f(*a, **kw)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m Py4JJavaError \u001b[94mas\u001b[0m e:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   │   \u001b[0mconverted = convert_exception(e.java_exception)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(converted, UnknownException):                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mprotocol.py\u001b[0m:\u001b[94m326\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_return_value\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m = answer[\u001b[94m1\u001b[0m]                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue = OUTPUT_CONVERTER[\u001b[96mtype\u001b[0m](answer[\u001b[94m2\u001b[0m:], gateway_client)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m answer[\u001b[94m1\u001b[0m] == REFERENCE_TYPE:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m326 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m Py4JJavaError(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m327 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mAn error occurred while calling \u001b[0m\u001b[33m{0}\u001b[0m\u001b[33m{1}\u001b[0m\u001b[33m{2}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m.                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m328 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mformat\u001b[0m(target_id, \u001b[33m\"\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m, name), value)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m329 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mPy4JJavaError: \u001b[0mAn error occurred while calling o540.csv.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task \u001b[1;36m0\u001b[0m in stage \u001b[1;36m2.0\u001b[0m failed \u001b[1;36m4\u001b[0m times, most \n",
       "recent failure: Lost task \u001b[1;36m0.3\u001b[0m in stage \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mTID \u001b[1;36m11\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;92m172.21.174.108\u001b[0m executor \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Driver stacktrace:\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2672\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2608\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ResizableArray.foreach\u001b[0m\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.mutable.ResizableArray.foreach$\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:55\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ArrayBuffer.foreach\u001b[0m\u001b[1m(\u001b[0mArrayBuffer.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.abortStage\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;36m1\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.foreach\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:407\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2860\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2802\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2791\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.EventLoop$$anon$\u001b[1;35m1.run\u001b[0m\u001b[1m(\u001b[0mEventLoop.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.runJob\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:952\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2249\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2268\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:506\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:459\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.CollectLimitExec.executeCollect\u001b[0m\u001b[1m(\u001b[0mlimit.scal\u001b[1;92ma:48\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.collectFromPlan\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3868\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3858\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.QueryExecution$\u001b[1;35m.withInternalError\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:510\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m6\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:109\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withSQLConfPropagated\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:169\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:95\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.SparkSession.withActive\u001b[0m\u001b[1m(\u001b[0mSparkSession.scal\u001b[1;92ma:779\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withNewExecutionId\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:64\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.withAction\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.head\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.take\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3084\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$\u001b[1;35m.infer\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:112\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:65\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVFileFormat.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$\u001b[1;35m11\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.orElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:447\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:207\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.resolveRelation\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:411\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.loadV1Source\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.getOrElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:189\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.csv\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:537\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.NativeMethodAccessorImpl.invoke0\u001b[0m\u001b[1m(\u001b[0mNative Method\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.NativeMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mNativeMethodAccessorImpl.jav\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.DelegatingMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mDelegatingMethodAccessorImpl.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.reflect.Method.invoke\u001b[0m\u001b[1m(\u001b[0mMethod.jav\u001b[1;92ma:498\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.MethodInvoker.invoke\u001b[0m\u001b[1m(\u001b[0mMethodInvoker.jav\u001b[1;92ma:244\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.ReflectionEngine.invoke\u001b[0m\u001b[1m(\u001b[0mReflectionEngine.jav\u001b[1;92ma:357\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.Gateway.invoke\u001b[0m\u001b[1m(\u001b[0mGateway.jav\u001b[1;92ma:282\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.AbstractCommand.invokeMethod\u001b[0m\u001b[1m(\u001b[0mAbstractCommand.jav\u001b[1;92ma:132\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.CallCommand.execute\u001b[0m\u001b[1m(\u001b[0mCallCommand.jav\u001b[1;92ma:79\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.waitForCommands\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.run\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:106\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[33m...\u001b[0m \u001b[1;36m1\u001b[0m more\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/coder/eromero/iris-pyspark/data/01_raw/iris.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7afb3168-9cb3-4aee-bbf4-50a536d4798d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd7cb90a-76ad-4534-84b1-9df6c5f6cb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/13/23 12:40:39] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Resolved project path as: <span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris-pyspark.</span>            <a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#139\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To set a different path, run <span style=\"color: #008000; text-decoration-color: #008000\">'%reload_kedro &lt;project_root&gt;'</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/13/23 12:40:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Resolved project path as: \u001b[35m/home/coder/eromero/\u001b[0m\u001b[95miris-pyspark.\u001b[0m            \u001b]8;id=164348;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=77961;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#139\u001b\\\u001b[2m139\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         To set a different path, run \u001b[32m'%reload_kedro \u001b[0m\u001b[32m<\u001b[0m\u001b[32mproject_root\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 12:40:40 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/13/23 12:40:41] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Kedro project iris-pyspark                                             <a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#108\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/13/23 12:40:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Kedro project iris-pyspark                                             \u001b]8;id=650917;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=126312;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#108\u001b\\\u001b[2m108\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Defined global variable <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'session'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'catalog'</span> and            <a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'pipelines'</span>                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Defined global variable \u001b[32m'context'\u001b[0m, \u001b[32m'session'\u001b[0m, \u001b[32m'catalog'\u001b[0m and            \u001b]8;id=775367;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=107727;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'pipelines'\u001b[0m                                                            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/13/23 12:40:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Registered line magic <span style=\"color: #008000; text-decoration-color: #008000\">'run_viz'</span>                                        <a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#115\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/13/23 12:40:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Registered line magic \u001b[32m'run_viz'\u001b[0m                                        \u001b]8;id=889086;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906677;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/ipython/__init__.py#115\u001b\\\u001b[2m115\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'iris_local'\u001b[0m, \u001b[32m'parameters'\u001b[0m, \u001b[32m'params:example_test_data_ratio'\u001b[0m, \u001b[32m'params:example_num_trees'\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_kedro\n",
    "catalog.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c725bb2b-ac5a-4f93-a108-f22abcbc55f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/13/23 12:40:50] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #008000; text-decoration-color: #008000\">'iris_local'</span> <span style=\"font-weight: bold\">(</span>SparkDataSet<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                   <a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/data_catalog.py#492\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">492</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/13/23 12:40:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[32m'iris_local'\u001b[0m \u001b[1m(\u001b[0mSparkDataSet\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                   \u001b]8;id=75850;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=371578;file:///home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/data_catalog.py#492\u001b\\\u001b[2m492\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 12:40:52 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.21.23.198 executor 0): java.io.FileNotFoundException: \n",
      "File file:/home/coder/eromero/iris-pyspark/data/01_raw/iris.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/09/13 12:40:53 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">core.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">192</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._logger.debug(<span style=\"color: #808000; text-decoration-color: #808000\">\"Loading %s\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>))                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">190 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>192 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._load()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> DatasetError:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> exc:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro_datasets/spark/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spark_dataset</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">397</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_load</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">394 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._schema:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">395 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>read_obj = read_obj.schema(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._schema)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">396 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>397 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> read_obj.load(load_path, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._file_format, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._load_args)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">398 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">399 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_save</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, data: DataFrame) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">400 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>save_path = _strip_dbfs_prefix(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._fs_prefix + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_save_path()))       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">readwriter.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">177</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.schema(schema)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 175 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.options(**options)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(path, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 177 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._df(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._jreader.load(path))                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 178 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> path <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 179 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(path) != <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 180 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>path = [path]  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[list-item]</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">java_gateway.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1321</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>proto.END_COMMAND_PART                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>answer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client.send_command(command)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1321 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_value = get_return_value(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>answer, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gateway_client, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.target_id, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> temp_arg <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> temp_args:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">190</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">capture_sql_exception</span>(f: Callable[..., Any]) -&gt; Callable[..., Any]:                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">deco</span>(*a: Any, **kw: Any) -&gt; Any:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>190 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*a, **kw)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> Py4JJavaError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>converted = convert_exception(e.java_exception)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(converted, UnknownException):                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">protocol.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">326</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_return_value</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span> = answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>value = OUTPUT_CONVERTER[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>](answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:], gateway_client)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">325 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> answer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] == REFERENCE_TYPE:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>326 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> Py4JJavaError(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">327 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"An error occurred while calling {0}{1}{2}.\\n\"</span>.                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">328 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">format</span>(target_id, <span style=\"color: #808000; text-decoration-color: #808000\">\".\"</span>, name), value)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">329 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Py4JJavaError: </span>An error occurred while calling o513.load.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> failed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> times, most \n",
       "recent failure: Lost task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> <span style=\"font-weight: bold\">(</span>TID <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">172.21.174.108</span> executor <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Driver stacktrace:\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2672</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2608</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ResizableArray.foreach</span><span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.mutable.ResizableArray.foreach$<span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:55</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ArrayBuffer.foreach</span><span style=\"font-weight: bold\">(</span>ArrayBuffer.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.abortStage</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.foreach</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:407</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2860</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2802</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2791</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.EventLoop$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.run</span><span style=\"font-weight: bold\">(</span>EventLoop.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.runJob</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:952</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2249</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2268</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:506</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:459</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.CollectLimitExec.executeCollect</span><span style=\"font-weight: bold\">(</span>limit.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:48</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.collectFromPlan</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3868</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3858</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.QueryExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withInternalError</span><span style=\"font-weight: bold\">(</span>QueryExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:510</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">6</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:109</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withSQLConfPropagated</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:169</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:95</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.SparkSession.withActive</span><span style=\"font-weight: bold\">(</span>SparkSession.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:779</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withNewExecutionId</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:64</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.withAction</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.head</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.take</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3084</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.infer</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:112</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:65</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVFileFormat.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">11</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.orElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:447</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:207</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.resolveRelation</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:411</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.loadV1Source</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.getOrElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:189</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:185</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.GeneratedMethodAccessor21.invoke</span><span style=\"font-weight: bold\">(</span>Unknown Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.DelegatingMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>DelegatingMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.reflect.Method.invoke</span><span style=\"font-weight: bold\">(</span>Method.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:498</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.MethodInvoker.invoke</span><span style=\"font-weight: bold\">(</span>MethodInvoker.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:244</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.ReflectionEngine.invoke</span><span style=\"font-weight: bold\">(</span>ReflectionEngine.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:357</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.Gateway.invoke</span><span style=\"font-weight: bold\">(</span>Gateway.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:282</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.AbstractCommand.invokeMethod</span><span style=\"font-weight: bold\">(</span>AbstractCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:132</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.CallCommand.execute</span><span style=\"font-weight: bold\">(</span>CallCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:79</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.waitForCommands</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.run</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:106</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> more\n",
       "\n",
       "\n",
       "<span style=\"font-style: italic\">The above exception was the direct cause of the following exception:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 iris_local = catalog.load(<span style=\"color: #808000; text-decoration-color: #808000\">'iris_local'</span>)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">data_catalog.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">496</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">493 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Loading data from '%s' (%s)...\"</span>, name, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(dataset).<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">494 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">495 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>496 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = dataset.load()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">497 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> result                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">499 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">core.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">613</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">610 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._filepath / version / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._filepath.name                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">611 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">612 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; _DO:  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># noqa: useless-parent-delegation</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>613 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().load()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">614 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">615 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, data: _DI) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._version_cache.clear()                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">core.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">201</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>message = (                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Failed while loading data from data set {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>)<span style=\"color: #808000; text-decoration-color: #808000\">}.\\n{</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(exc)<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>201 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> DatasetError(message) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">exc</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, data: _DI) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">204 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Saves data by delegation to the provided save method.</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">DatasetError: </span>Failed while loading data from data set <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SparkDataSet</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">file_format</span>=<span style=\"color: #800080; text-decoration-color: #800080\">csv</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">filepath</span>=<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span>, <span style=\"color: #808000; text-decoration-color: #808000\">load_args</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'header'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inferSchema'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">save_args</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'header'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sep'</span>: ,<span style=\"font-weight: bold\">})</span>.\n",
       "An error occurred while calling o513.load.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> failed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> times, most \n",
       "recent failure: Lost task <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span> in stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> <span style=\"font-weight: bold\">(</span>TID <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">172.21.174.108</span> executor <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Driver stacktrace:\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2672</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2608</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ResizableArray.foreach</span><span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.mutable.ResizableArray.foreach$<span style=\"font-weight: bold\">(</span>ResizableArray.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:55</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.collection.mutable.ArrayBuffer.foreach</span><span style=\"font-weight: bold\">(</span>ArrayBuffer.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.abortStage</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2607</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.foreach</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:407</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2860</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2802</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2791</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.EventLoop$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.run</span><span style=\"font-weight: bold\">(</span>EventLoop.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:49</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.DAGScheduler.runJob</span><span style=\"font-weight: bold\">(</span>DAGScheduler.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:952</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2249</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.SparkContext.runJob</span><span style=\"font-weight: bold\">(</span>SparkContext.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2268</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:506</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.SparkPlan.executeTake</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:459</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.CollectLimitExec.executeCollect</span><span style=\"font-weight: bold\">(</span>limit.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:48</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.collectFromPlan</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3868</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3858</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.QueryExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withInternalError</span><span style=\"font-weight: bold\">(</span>QueryExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:510</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">6</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:109</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withSQLConfPropagated</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:169</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:95</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.SparkSession.withActive</span><span style=\"font-weight: bold\">(</span>SparkSession.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:779</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SQLExecution$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.withNewExecutionId</span><span style=\"font-weight: bold\">(</span>SQLExecution.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:64</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.withAction</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3856</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.head</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:2863</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.Dataset.take</span><span style=\"font-weight: bold\">(</span>Dataset.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:3084</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.infer</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:112</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVDataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:65</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema</span><span style=\"font-weight: bold\">(</span>CSVFileFormat.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:62</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">11</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.orElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:447</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:207</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.datasources.DataSource.resolveRelation</span><span style=\"font-weight: bold\">(</span>DataSource.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:411</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.loadV1Source</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:228</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scala.Option.getOrElse</span><span style=\"font-weight: bold\">(</span>Option.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:189</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:210</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.DataFrameReader.load</span><span style=\"font-weight: bold\">(</span>DataFrameReader.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:185</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.GeneratedMethodAccessor21.invoke</span><span style=\"font-weight: bold\">(</span>Unknown Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sun.reflect.DelegatingMethodAccessorImpl.invoke</span><span style=\"font-weight: bold\">(</span>DelegatingMethodAccessorImpl.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.reflect.Method.invoke</span><span style=\"font-weight: bold\">(</span>Method.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:498</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.MethodInvoker.invoke</span><span style=\"font-weight: bold\">(</span>MethodInvoker.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:244</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.reflection.ReflectionEngine.invoke</span><span style=\"font-weight: bold\">(</span>ReflectionEngine.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:357</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.Gateway.invoke</span><span style=\"font-weight: bold\">(</span>Gateway.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:282</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.AbstractCommand.invokeMethod</span><span style=\"font-weight: bold\">(</span>AbstractCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:132</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.commands.CallCommand.execute</span><span style=\"font-weight: bold\">(</span>CallCommand.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:79</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.waitForCommands</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:182</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">py4j.ClientServerConnection.run</span><span style=\"font-weight: bold\">(</span>ClientServerConnection.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:106</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.lang.Thread.run</span><span style=\"font-weight: bold\">(</span>Thread.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:750</span><span style=\"font-weight: bold\">)</span>\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:<span style=\"color: #800080; text-decoration-color: #800080\">/home/coder/eromero/iris-pyspark/data/01_raw/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">iris.csv</span> does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running <span style=\"color: #008000; text-decoration-color: #008000\">'REFRESH TABLE tableName'</span> command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.readCurrentFileNotFoundError</span><span style=\"font-weight: bold\">(</span>QueryExecutionErrors.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:661</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>org$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">readCurrentFile</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:212</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.nextIterator</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:270</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>FileScanRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:116</span><span style=\"font-weight: bold\">)</span>\n",
       "        at scala.collection.Iterator$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">10.hasNext</span><span style=\"font-weight: bold\">(</span>Iterator.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:460</span><span style=\"font-weight: bold\">)</span>\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratedIteratorForCodegenStage1.processNext</span><span style=\"font-weight: bold\">(</span>Unknown \n",
       "Source<span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.sql.execution.BufferedRowIterator.hasNext</span><span style=\"font-weight: bold\">(</span>BufferedRowIterator.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:43</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1.hasNext</span><span style=\"font-weight: bold\">(</span>WholeStageCodegenExec.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:760</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1</span><span style=\"font-weight: bold\">(</span>SparkPlan.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:364</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">2</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">adapted</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:890</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.MapPartitionsRDD.compute</span><span style=\"font-weight: bold\">(</span>MapPartitionsRDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:52</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.computeOrReadCheckpoint</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:365</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.rdd.RDD.iterator</span><span style=\"font-weight: bold\">(</span>RDD.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:329</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.ResultTask.runTask</span><span style=\"font-weight: bold\">(</span>ResultTask.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:90</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">org.apache.spark.scheduler.Task.run</span><span style=\"font-weight: bold\">(</span>Task.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:136</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">3</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:548</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.util.Utils$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.tryWithSafeFinally</span><span style=\"font-weight: bold\">(</span>Utils.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1504</span><span style=\"font-weight: bold\">)</span>\n",
       "        at org.apache.spark.executor.Executor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TaskRunner.run</span><span style=\"font-weight: bold\">(</span>Executor.scal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:551</span><span style=\"font-weight: bold\">)</span>\n",
       "        at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">java.util.concurrent.ThreadPoolExecutor.runWorker</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:1149</span><span style=\"font-weight: bold\">)</span>\n",
       "        at java.util.concurrent.ThreadPoolExecutor$<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Worker.run</span><span style=\"font-weight: bold\">(</span>ThreadPoolExecutor.jav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">a:624</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> more\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m192\u001b[0m in \u001b[92mload\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._logger.debug(\u001b[33m\"\u001b[0m\u001b[33mLoading \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, \u001b[96mstr\u001b[0m(\u001b[96mself\u001b[0m))                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m190 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m192 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._load()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m DatasetError:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m exc:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro_datasets/spark/\u001b[0m\u001b[1;33mspark_dataset\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m397\u001b[0m in \u001b[92m_load\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m394 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._schema:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m395 \u001b[0m\u001b[2m│   │   │   \u001b[0mread_obj = read_obj.schema(\u001b[96mself\u001b[0m._schema)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m396 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m397 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m read_obj.load(load_path, \u001b[96mself\u001b[0m._file_format, **\u001b[96mself\u001b[0m._load_args)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m398 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m399 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_save\u001b[0m(\u001b[96mself\u001b[0m, data: DataFrame) -> \u001b[94mNone\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   \u001b[0msave_path = _strip_dbfs_prefix(\u001b[96mself\u001b[0m._fs_prefix + \u001b[96mstr\u001b[0m(\u001b[96mself\u001b[0m._get_save_path()))       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mreadwriter.py\u001b[0m:\u001b[94m177\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mload\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 174 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.schema(schema)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 175 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.options(**options)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 176 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(path, \u001b[96mstr\u001b[0m):                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 177 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._df(\u001b[96mself\u001b[0m._jreader.load(path))                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 178 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m path \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 179 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mtype\u001b[0m(path) != \u001b[96mlist\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 180 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath = [path]  \u001b[2m# type: ignore[list-item]\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mjava_gateway.py\u001b[0m:\u001b[94m1321\u001b[0m in       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1318 \u001b[0m\u001b[2m│   │   │   \u001b[0mproto.END_COMMAND_PART                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1319 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1320 \u001b[0m\u001b[2m│   │   \u001b[0manswer = \u001b[96mself\u001b[0m.gateway_client.send_command(command)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1321 \u001b[2m│   │   \u001b[0mreturn_value = get_return_value(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   │   \u001b[0manswer, \u001b[96mself\u001b[0m.gateway_client, \u001b[96mself\u001b[0m.target_id, \u001b[96mself\u001b[0m.name)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m temp_arg \u001b[95min\u001b[0m temp_args:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/pyspark/sql/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m190\u001b[0m in \u001b[92mdeco\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcapture_sql_exception\u001b[0m(f: Callable[..., Any]) -> Callable[..., Any]:                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdeco\u001b[0m(*a: Any, **kw: Any) -> Any:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m190 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m f(*a, **kw)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m Py4JJavaError \u001b[94mas\u001b[0m e:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   │   \u001b[0mconverted = convert_exception(e.java_exception)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(converted, UnknownException):                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/py4j/\u001b[0m\u001b[1;33mprotocol.py\u001b[0m:\u001b[94m326\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_return_value\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m = answer[\u001b[94m1\u001b[0m]                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue = OUTPUT_CONVERTER[\u001b[96mtype\u001b[0m](answer[\u001b[94m2\u001b[0m:], gateway_client)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m answer[\u001b[94m1\u001b[0m] == REFERENCE_TYPE:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m326 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m Py4JJavaError(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m327 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mAn error occurred while calling \u001b[0m\u001b[33m{0}\u001b[0m\u001b[33m{1}\u001b[0m\u001b[33m{2}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m.                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m328 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mformat\u001b[0m(target_id, \u001b[33m\"\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m, name), value)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m329 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mPy4JJavaError: \u001b[0mAn error occurred while calling o513.load.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task \u001b[1;36m0\u001b[0m in stage \u001b[1;36m0.0\u001b[0m failed \u001b[1;36m4\u001b[0m times, most \n",
       "recent failure: Lost task \u001b[1;36m0.3\u001b[0m in stage \u001b[1;36m0.0\u001b[0m \u001b[1m(\u001b[0mTID \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;92m172.21.174.108\u001b[0m executor \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Driver stacktrace:\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2672\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2608\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ResizableArray.foreach\u001b[0m\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.mutable.ResizableArray.foreach$\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:55\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ArrayBuffer.foreach\u001b[0m\u001b[1m(\u001b[0mArrayBuffer.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.abortStage\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;36m1\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.foreach\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:407\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2860\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2802\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2791\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.EventLoop$$anon$\u001b[1;35m1.run\u001b[0m\u001b[1m(\u001b[0mEventLoop.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.runJob\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:952\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2249\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2268\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:506\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:459\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.CollectLimitExec.executeCollect\u001b[0m\u001b[1m(\u001b[0mlimit.scal\u001b[1;92ma:48\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.collectFromPlan\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3868\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3858\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.QueryExecution$\u001b[1;35m.withInternalError\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:510\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m6\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:109\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withSQLConfPropagated\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:169\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:95\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.SparkSession.withActive\u001b[0m\u001b[1m(\u001b[0mSparkSession.scal\u001b[1;92ma:779\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withNewExecutionId\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:64\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.withAction\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.head\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.take\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3084\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$\u001b[1;35m.infer\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:112\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:65\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVFileFormat.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$\u001b[1;35m11\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.orElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:447\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:207\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.resolveRelation\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:411\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.loadV1Source\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.getOrElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:189\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:185\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.GeneratedMethodAccessor21.invoke\u001b[0m\u001b[1m(\u001b[0mUnknown Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.DelegatingMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mDelegatingMethodAccessorImpl.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.reflect.Method.invoke\u001b[0m\u001b[1m(\u001b[0mMethod.jav\u001b[1;92ma:498\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.MethodInvoker.invoke\u001b[0m\u001b[1m(\u001b[0mMethodInvoker.jav\u001b[1;92ma:244\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.ReflectionEngine.invoke\u001b[0m\u001b[1m(\u001b[0mReflectionEngine.jav\u001b[1;92ma:357\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.Gateway.invoke\u001b[0m\u001b[1m(\u001b[0mGateway.jav\u001b[1;92ma:282\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.AbstractCommand.invokeMethod\u001b[0m\u001b[1m(\u001b[0mAbstractCommand.jav\u001b[1;92ma:132\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.CallCommand.execute\u001b[0m\u001b[1m(\u001b[0mCallCommand.jav\u001b[1;92ma:79\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.waitForCommands\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.run\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:106\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[33m...\u001b[0m \u001b[1;36m1\u001b[0m more\n",
       "\n",
       "\n",
       "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
       "\n",
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 iris_local = catalog.load(\u001b[33m'\u001b[0m\u001b[33miris_local\u001b[0m\u001b[33m'\u001b[0m)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/\u001b[0m\u001b[1;33mdata_catalog.py\u001b[0m:\u001b[94m496\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mload\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m493 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading data from \u001b[0m\u001b[33m'\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m'\u001b[0m\u001b[33m (\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m)...\u001b[0m\u001b[33m\"\u001b[0m, name, \u001b[96mtype\u001b[0m(dataset).\u001b[91m__name__\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m494 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m495 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m496 \u001b[2m│   │   \u001b[0mresult = dataset.load()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m497 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m result                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m499 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m613\u001b[0m in \u001b[92mload\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m610 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._filepath / version / \u001b[96mself\u001b[0m._filepath.name                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m611 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m612 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mload\u001b[0m(\u001b[96mself\u001b[0m) -> _DO:  \u001b[2m# noqa: useless-parent-delegation\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m613 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().load()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m614 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m615 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92msave\u001b[0m(\u001b[96mself\u001b[0m, data: _DI) -> \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m616 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._version_cache.clear()                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/coder/miniconda3/envs/demml/lib/python3.8/site-packages/kedro/io/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m201\u001b[0m in \u001b[92mload\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   │   \u001b[0mmessage = (                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFailed while loading data from data set \u001b[0m\u001b[33m{\u001b[0m\u001b[96mstr\u001b[0m(\u001b[96mself\u001b[0m)\u001b[33m}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0m\u001b[96mstr\u001b[0m(exc)\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m DatasetError(message) \u001b[94mfrom\u001b[0m \u001b[4;96mexc\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92msave\u001b[0m(\u001b[96mself\u001b[0m, data: _DI) -> \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Saves data by delegation to the provided save method.\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mDatasetError: \u001b[0mFailed while loading data from data set \u001b[1;35mSparkDataSet\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfile_format\u001b[0m=\u001b[35mcsv\u001b[0m, \n",
       "\u001b[33mfilepath\u001b[0m=\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m, \u001b[33mload_args\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'header'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'inferSchema'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33msave_args\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'header'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'sep'\u001b[0m: ,\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m.\n",
       "An error occurred while calling o513.load.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task \u001b[1;36m0\u001b[0m in stage \u001b[1;36m0.0\u001b[0m failed \u001b[1;36m4\u001b[0m times, most \n",
       "recent failure: Lost task \u001b[1;36m0.3\u001b[0m in stage \u001b[1;36m0.0\u001b[0m \u001b[1m(\u001b[0mTID \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;92m172.21.174.108\u001b[0m executor \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Driver stacktrace:\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2672\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2608\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ResizableArray.foreach\u001b[0m\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.mutable.ResizableArray.foreach$\u001b[1m(\u001b[0mResizableArray.scal\u001b[1;92ma:55\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.collection.mutable.ArrayBuffer.foreach\u001b[0m\u001b[1m(\u001b[0mArrayBuffer.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.abortStage\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2607\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$\u001b[1;36m1\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.foreach\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:407\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:1182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2860\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2802\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:2791\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.EventLoop$$anon$\u001b[1;35m1.run\u001b[0m\u001b[1m(\u001b[0mEventLoop.scal\u001b[1;92ma:49\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.DAGScheduler.runJob\u001b[0m\u001b[1m(\u001b[0mDAGScheduler.scal\u001b[1;92ma:952\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2249\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.SparkContext.runJob\u001b[0m\u001b[1m(\u001b[0mSparkContext.scal\u001b[1;92ma:2268\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:506\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.SparkPlan.executeTake\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:459\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.CollectLimitExec.executeCollect\u001b[0m\u001b[1m(\u001b[0mlimit.scal\u001b[1;92ma:48\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.collectFromPlan\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3868\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$head$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3858\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.QueryExecution$\u001b[1;35m.withInternalError\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:510\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.Dataset.$anonfun$withAction$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m6\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:109\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withSQLConfPropagated\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:169\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:95\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.SparkSession.withActive\u001b[0m\u001b[1m(\u001b[0mSparkSession.scal\u001b[1;92ma:779\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withNewExecutionId\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:64\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.withAction\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3856\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.head\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:2863\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.Dataset.take\u001b[0m\u001b[1m(\u001b[0mDataset.scal\u001b[1;92ma:3084\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$\u001b[1;35m.infer\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:112\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVDataSource.scal\u001b[1;92ma:65\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema\u001b[0m\u001b[1m(\u001b[0mCSVFileFormat.scal\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$\u001b[1;35m11\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.orElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:447\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:207\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.datasources.DataSource.resolveRelation\u001b[0m\u001b[1m(\u001b[0mDataSource.scal\u001b[1;92ma:411\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.loadV1Source\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:228\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.DataFrameReader.$anonfun$load$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mscala.Option.getOrElse\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:189\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:210\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.DataFrameReader.load\u001b[0m\u001b[1m(\u001b[0mDataFrameReader.scal\u001b[1;92ma:185\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.GeneratedMethodAccessor21.invoke\u001b[0m\u001b[1m(\u001b[0mUnknown Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35msun.reflect.DelegatingMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mDelegatingMethodAccessorImpl.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.reflect.Method.invoke\u001b[0m\u001b[1m(\u001b[0mMethod.jav\u001b[1;92ma:498\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.MethodInvoker.invoke\u001b[0m\u001b[1m(\u001b[0mMethodInvoker.jav\u001b[1;92ma:244\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.reflection.ReflectionEngine.invoke\u001b[0m\u001b[1m(\u001b[0mReflectionEngine.jav\u001b[1;92ma:357\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.Gateway.invoke\u001b[0m\u001b[1m(\u001b[0mGateway.jav\u001b[1;92ma:282\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.AbstractCommand.invokeMethod\u001b[0m\u001b[1m(\u001b[0mAbstractCommand.jav\u001b[1;92ma:132\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.commands.CallCommand.execute\u001b[0m\u001b[1m(\u001b[0mCallCommand.jav\u001b[1;92ma:79\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.waitForCommands\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:182\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mpy4j.ClientServerConnection.run\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:106\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:750\u001b[0m\u001b[1m)\u001b[0m\n",
       "Caused by: java.io.FileNotFoundException: \n",
       "File file:\u001b[35m/home/coder/eromero/iris-pyspark/data/01_raw/\u001b[0m\u001b[95miris.csv\u001b[0m does not exist\n",
       "\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate\n",
       "the cache in Spark by running \u001b[32m'REFRESH TABLE tableName'\u001b[0m command in SQL or by\n",
       "recreating the Dataset/DataFrame involved.\n",
       "       \n",
       "        at \n",
       "org.apache.spark.sql.errors.QueryExecutionErrors$\u001b[1;35m.readCurrentFileNotFoundError\u001b[0m\u001b[1m(\u001b[0mQueryExecutionErrors.scal\u001b[1;92ma:661\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;36m1.\u001b[0morg$apache$spark$sql$execution$datasources$FileScanR\n",
       "DD$$anon$$\u001b[1;35mreadCurrentFile\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:212\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.nextIterator\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:270\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mFileScanRDD.scal\u001b[1;92ma:116\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at scala.collection.Iterator$$anon$\u001b[1;35m10.hasNext\u001b[0m\u001b[1m(\u001b[0mIterator.scal\u001b[1;92ma:460\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \n",
       "org.apache.spark.sql.catalyst.expressions.GeneratedClass$\u001b[1;35mGeneratedIteratorForCodegenStage1.processNext\u001b[0m\u001b[1m(\u001b[0mUnknown \n",
       "Source\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.sql.execution.BufferedRowIterator.hasNext\u001b[0m\u001b[1m(\u001b[0mBufferedRowIterator.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$\u001b[1;35m1.hasNext\u001b[0m\u001b[1m(\u001b[0mWholeStageCodegenExec.scal\u001b[1;92ma:760\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSparkPlan.scal\u001b[1;92ma:364\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;35m2\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$\u001b[1;36m2\u001b[0m$\u001b[1;35madapted\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:890\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.MapPartitionsRDD.compute\u001b[0m\u001b[1m(\u001b[0mMapPartitionsRDD.scal\u001b[1;92ma:52\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.computeOrReadCheckpoint\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:365\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.rdd.RDD.iterator\u001b[0m\u001b[1m(\u001b[0mRDD.scal\u001b[1;92ma:329\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.ResultTask.runTask\u001b[0m\u001b[1m(\u001b[0mResultTask.scal\u001b[1;92ma:90\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35morg.apache.spark.scheduler.Task.run\u001b[0m\u001b[1m(\u001b[0mTask.scal\u001b[1;92ma:136\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$\u001b[1;35m3\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:548\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.util.Utils$\u001b[1;35m.tryWithSafeFinally\u001b[0m\u001b[1m(\u001b[0mUtils.scal\u001b[1;92ma:1504\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at org.apache.spark.executor.Executor$\u001b[1;35mTaskRunner.run\u001b[0m\u001b[1m(\u001b[0mExecutor.scal\u001b[1;92ma:551\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at \u001b[1;35mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:1149\u001b[0m\u001b[1m)\u001b[0m\n",
       "        at java.util.concurrent.ThreadPoolExecutor$\u001b[1;35mWorker.run\u001b[0m\u001b[1m(\u001b[0mThreadPoolExecutor.jav\u001b[1;92ma:624\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[33m...\u001b[0m \u001b[1;36m1\u001b[0m more\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris_local = catalog.load('iris_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c183fc31-5985-4edd-b4e2-5e8ca1b87eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 iris_local.show()                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'iris_local'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 iris_local.show()                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'iris_local'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris_local.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0721db3-8841-45b5-9781-9bac205115da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (iris_pyspark)",
   "language": "python",
   "name": "kedro_iris_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
